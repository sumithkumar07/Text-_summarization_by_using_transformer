{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPRP9s0TKahXbo/dbh+1e/m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumithkumar07/Text-_summarization_by_using_transformer/blob/main/Text__summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ycDuh-HHsVA"
      },
      "outputs": [],
      "source": [
        "pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --upgrade"
      ],
      "metadata": {
        "id": "6muzPfCNQlMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/nileshmalode1/samsum-dataset-text-summarization\")"
      ],
      "metadata": {
        "id": "vHh6qW3pIBvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "xmCilIfLIpVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"/content/samsum-dataset-text-summarization/samsum-train.csv\")\n",
        "\n",
        "validation_data = pd.read_csv(\"/content/samsum-dataset-text-summarization/samsum-validation.csv\")"
      ],
      "metadata": {
        "id": "N8IB2jA-IOi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.sample(n=6000,random_state=42).reset_index(drop=True)\n",
        "validation_data = validation_data.sample(n=500, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "5Cy9wtQaymKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        text = re.sub(r'\\r\\n', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'<.*?>', '', text)\n",
        "        text = text.strip().lower()\n",
        "    else:\n",
        "        text = str(text)\n",
        "    return text\n",
        "\n",
        "train_data['dialogue'] = train_data['dialogue'].apply(clean_text)\n",
        "train_data['summary'] = train_data['summary'].apply(clean_text)\n",
        "\n",
        "validation_data['dialogue'] = validation_data['dialogue'].apply(clean_text)\n",
        "validation_data['summary'] = validation_data['summary'].apply(clean_text)\n",
        "\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "Fc8v3vu2K65v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "0XHabutZLNyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples[\"dialogue\"], padding = 'max_length', truncation=True, max_length=512)\n",
        "    targets = tokenizer(examples[\"summary\"], padding = 'max_length', truncation=True, max_length=150)\n",
        "    inputs['labels'] = targets['input_ids']\n",
        "    return inputs\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = train_data.apply(preprocess_function, axis=1)\n",
        "validation_data = validation_data.apply(preprocess_function, axis=1)"
      ],
      "metadata": {
        "id": "02PBP1mlL52F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "RcgRAS_aNPj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "w-LZycwYNmSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = './results',\n",
        "    num_train_epochs = 10,\n",
        "    per_device_train_batch_size = 16,\n",
        "    per_device_eval_batch_size = 16,\n",
        "    warmup_steps = 500,\n",
        "    weight_decay = 0.01,\n",
        "    logging_dir = './logs',\n",
        "    logging_steps = 50,\n",
        "    save_steps = 500,\n",
        "    eval_steps = 50,\n",
        "\n",
        "    eval_strategy = \"epoch\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = validation_data\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "OBKJJrHXQsX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WNOkZHDghgBn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}